#!/bin/bash
# Backup to GCP Cloud Storage
# Cost: $0.020/GB/month (cheaper than S3!)
# 
# Setup:
# 1. Create GCP bucket: gsutil mb gs://expenseflow-backups
# 2. Set credentials: export GOOGLE_APPLICATION_CREDENTIALS=/path/to/gcp-key.json
# 3. Run: railway run ./scripts/backup_to_gcp.sh

set -e

# Configuration
GCP_BUCKET="expense-flow-backups"  # Your GCP bucket name
DATE=$(date +%Y-%m-%d_%H-%M-%S)
BACKUP_FILE="/tmp/expenseflow_$DATE.sql.gz"

echo "üîÑ Starting GCP backup at $DATE..."

# Check if gsutil is installed
if ! command -v gsutil &> /dev/null; then
    echo "‚ùå gsutil not found. Install: pip install gsutil"
    exit 1
fi

# Check DATABASE_PUBLIC_URL
if [ -z "$DATABASE_PUBLIC_URL" ]; then
    echo "‚ùå DATABASE_PUBLIC_URL not set. Run: railway run ./scripts/backup_to_gcp.sh"
    exit 1
fi

# Use PostgreSQL 17 pg_dump to match Railway's version
if [ -f "/opt/homebrew/opt/postgresql@17/bin/pg_dump" ]; then
    PG_DUMP="/opt/homebrew/opt/postgresql@17/bin/pg_dump"
elif [ -f "/usr/local/opt/postgresql@17/bin/pg_dump" ]; then
    PG_DUMP="/usr/local/opt/postgresql@17/bin/pg_dump"
else
    PG_DUMP="pg_dump"
fi

# Create backup
echo "üì¶ Dumping database..."
$PG_DUMP "$DATABASE_PUBLIC_URL" | gzip > "$BACKUP_FILE"

# Get file size
SIZE=$(du -h "$BACKUP_FILE" | cut -f1)
echo "üìä Backup size: $SIZE"

# Upload to GCP Cloud Storage
echo "‚òÅÔ∏è  Uploading to GCP Cloud Storage..."
gsutil cp "$BACKUP_FILE" "gs://$GCP_BUCKET/backups/$(basename $BACKUP_FILE)"

# Clean up local file
rm "$BACKUP_FILE"

# Delete backups older than 30 days (save money)
echo "üßπ Cleaning old GCP backups (keeping last 30 days)..."
gsutil ls "gs://$GCP_BUCKET/backups/" | while read file; do
    # Extract date from filename (format: expenseflow_YYYY-MM-DD_HH-MM-SS.sql.gz)
    filename=$(basename "$file")
    file_date=$(echo "$filename" | grep -oE '[0-9]{4}-[0-9]{2}-[0-9]{2}' | head -1)
    
    if [ -n "$file_date" ]; then
        # Calculate age in days
        if [[ "$OSTYPE" == "darwin"* ]]; then
            # macOS
            file_epoch=$(date -j -f "%Y-%m-%d" "$file_date" +%s 2>/dev/null || echo 0)
        else
            # Linux
            file_epoch=$(date -d "$file_date" +%s 2>/dev/null || echo 0)
        fi
        
        now_epoch=$(date +%s)
        days_old=$(( ($now_epoch - $file_epoch) / 86400 ))
        
        if [ $days_old -gt 30 ]; then
            echo "  Deleting old backup: $filename (${days_old} days old)"
            gsutil rm "$file"
        fi
    fi
done

echo ""
echo "‚úÖ Backup complete!"
echo "üìç Location: gs://$GCP_BUCKET/backups/expenseflow_$DATE.sql.gz"
echo ""

# List recent backups
echo "üìä Recent backups:"
gsutil ls -lh "gs://$GCP_BUCKET/backups/" | tail -5

echo ""
echo "üí° To restore:"
echo "   gsutil cp gs://$GCP_BUCKET/backups/expenseflow_$DATE.sql.gz - | gunzip | railway run psql \$DATABASE_URL"
