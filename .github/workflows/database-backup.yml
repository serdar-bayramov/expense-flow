name: Database Backup to GCP

on:
  schedule:
    # Run daily at 3 AM UTC
    - cron: '0 3 * * *'
  workflow_dispatch:  # Allow manual trigger

jobs:
  backup:
    runs-on: ubuntu-latest
    
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
      
      - name: Install PostgreSQL 17 client
        run: |
          # Add PostgreSQL 17 repository
          sudo sh -c 'echo "deb http://apt.postgresql.org/pub/repos/apt $(lsb_release -cs)-pgdg main" > /etc/apt/sources.list.d/pgdg.list'
          wget --quiet -O - https://www.postgresql.org/media/keys/ACCC4CF8.asc | sudo apt-key add -
          sudo apt-get update
          sudo apt-get install -y postgresql-client-17
      
      - name: Set up Cloud SDK
        uses: google-github-actions/setup-gcloud@v2
        with:
          service_account_key: ${{ secrets.GCP_SA_KEY }}
          project_id: ${{ secrets.GCP_PROJECT_ID }}
      
      - name: Create backup
        env:
          DATABASE_URL: ${{ secrets.RAILWAY_DATABASE_URL }}
          GCP_BUCKET: expense-flow-backups
        run: |
          DATE=$(date +%Y-%m-%d_%H-%M-%S)
          BACKUP_FILE="expenseflow_$DATE.sql.gz"
          
          echo "üì¶ Creating backup: $BACKUP_FILE"
          echo "Using PostgreSQL version:"
          pg_dump --version
          
          # Use pg_dump 17 to match Railway
          pg_dump "$DATABASE_URL" | gzip > "$BACKUP_FILE"
          
          # Verify backup was created
          if [ ! -f "$BACKUP_FILE" ]; then
            echo "‚ùå Backup failed!"
            exit 1
          fi
          
          SIZE=$(du -h "$BACKUP_FILE" | cut -f1)
          echo "‚úÖ Backup created: $BACKUP_FILE ($SIZE)"
          
          # Upload to GCP
          echo "‚òÅÔ∏è  Uploading to GCP Cloud Storage..."
          gsutil cp "$BACKUP_FILE" "gs://$GCP_BUCKET/backups/$BACKUP_FILE"
          echo "‚úÖ Uploaded to gs://$GCP_BUCKET/backups/$BACKUP_FILE"
          
          # Clean up old GCP backups (keep last 30 days)
          echo "üßπ Cleaning old backups (older than 30 days)..."
          CUTOFF_DATE=$(date -u -d '30 days ago' +%Y-%m-%d)
          gsutil ls "gs://$GCP_BUCKET/backups/" | while read file; do
            FILE_DATE=$(echo "$file" | grep -o '[0-9]\{4\}-[0-9]\{2\}-[0-9]\{2\}' | head -1)
            if [[ "$FILE_DATE" < "$CUTOFF_DATE" ]]; then
              echo "  Deleting old backup: $file"
              gsutil rm "$file" || true
            fi
          done
          
          # Also upload as artifact (for easy access)
          echo "BACKUP_FILE=$BACKUP_FILE" >> $GITHUB_ENV
      
      - name: Upload backup as artifact (last 7 days)
        uses: actions/upload-artifact@v4
        with:
          name: database-backup-${{ github.run_id }}
          path: expenseflow_*.sql.gz
          retention-days: 7  # Short retention, GCP is primary storage
      
      - name: Clean old GCP backups (keep 30 days)
        env:
          GCP_BUCKET: expenseflow-backups
        run: |
          echo "üßπ Cleaning old backups..."
          gsutil ls "gs://$GCP_BUCKET/backups/" | while read file; do
            filename=$(basename "$file")
            file_date=$(echo "$filename" | grep -oE '[0-9]{4}-[0-9]{2}-[0-9]{2}' | head -1)
            
            if [ -n "$file_date" ]; then
              file_epoch=$(date -d "$file_date" +%s 2>/dev/null || echo 0)
              now_epoch=$(date +%s)
              days_old=$(( ($now_epoch - $file_epoch) / 86400 ))
              
              if [ $days_old -gt 30 ]; then
                echo "Deleting: $filename (${days_old} days old)"
                gsutil rm "$file"
              fi
            fi
          done
      
      - name: Notify on failure
        if: failure()
        run: |
          echo "‚ùå Backup failed! Check logs."
          # Add Slack/Discord notification here if needed
